{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercícios Pyspark & SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/06 14:01:03 WARN Utils: Your hostname, daniel-Lenovo-ideapad-330-15IKB resolves to a loopback address: 127.0.1.1; using 10.0.0.108 instead (on interface wlp2s0)\n",
      "24/06/06 14:01:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/06 14:01:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/06/06 14:01:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, lit, rand\n",
    "from pyspark.sql.types import StringType\n",
    "import random\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Exercicio Intro\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ler .txt e listar os 5 primeiros nomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `genero` cannot be resolved. Did you mean one of the following? [`_c0`].;\n'Filter 'genero IN (Drama,Romance,Drama,Romance)\n+- Relation [_c0#56] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m valores \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDrama,Romance\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDrama\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRomance\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m condicao \u001b[38;5;241m=\u001b[39m col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenero\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39misin(valores)\n\u001b[0;32m----> 7\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondicao\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m df(col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenero\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:3329\u001b[0m, in \u001b[0;36mDataFrame.filter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   3327\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mfilter(condition)\n\u001b[1;32m   3328\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(condition, Column):\n\u001b[0;32m-> 3329\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   3332\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN_OR_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3333\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcondition\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(condition)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   3334\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `genero` cannot be resolved. Did you mean one of the following? [`_c0`].;\n'Filter 'genero IN (Drama,Romance,Drama,Romance)\n+- Relation [_c0#56] csv\n"
     ]
    }
   ],
   "source": [
    "df_nomes = spark.read.csv('nomes_aleatorios.txt')\n",
    "df_nomes.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Alterar nome da coluna do DataFrame para nomes e apresentar as 10 primeiras linhas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      "\n",
      "+-----------------+\n",
      "|            Nomes|\n",
      "+-----------------+\n",
      "|   Frances Bennet|\n",
      "|    Jamie Russell|\n",
      "|   Edward Kistler|\n",
      "|    Sheila Maurer|\n",
      "| Donald Golightly|\n",
      "|       David Gray|\n",
      "|      Joy Bennett|\n",
      "|      Paul Kriese|\n",
      "|Berniece Ornellas|\n",
      "|    Brian Farrell|\n",
      "+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_nomes.printSchema()\n",
    "df_nomes = df_nomes.withColumnRenamed(\"_c0\", \"Nomes\")\n",
    "df_nomes.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Atribuindo ao DataFrame, a coluna Escolaridade, a coluna Pais e a coluna AnoNascimento, todas geradas com valores aleatórios, a partir de determinado intervalo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "niveis_escolaridade = ['Fundamental', 'Médio', 'Superior']\n",
    "paises_america_do_sul = [\"Argentina\",\"Bolívia\",\"Brasil\",\"Chile\",\"Colômbia\",\"Equador\",\"Guiana\",\"Paraguai\",\"Peru\",\"Suriname\",\"Uruguai\",\"Venezuela\",\"Guiana Francesa\"]\n",
    "\n",
    "\n",
    "def sortear_string(array):\n",
    "    return random.choice(array)\n",
    "\n",
    "\n",
    "sortear_string_udf = udf(sortear_string, StringType())\n",
    "\n",
    "df_nomes = df_nomes.withColumn(\"Escolaridade\", sortear_string_udf(lit(niveis_escolaridade)))\n",
    "df_nomes = df_nomes.withColumn(\"Pais\", sortear_string_udf(lit(paises_america_do_sul)))\n",
    "df_nomes = df_nomes.withColumn(\"AnoNascimento\", (1945 + (rand() * (2010 - 1945)).cast(\"int\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Trazer linhas do DataFrame com pessoas do século 2000 utilizando pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+---------------+-------------+\n",
      "|           Nomes|Escolaridade|           Pais|AnoNascimento|\n",
      "+----------------+------------+---------------+-------------+\n",
      "|     Joy Bennett|       Médio|         Guiana|         2007|\n",
      "| Howard Lazarine| Fundamental|         Guiana|         2003|\n",
      "|    Leroy Strahl|       Médio|        Bolívia|         2008|\n",
      "|    David Medina|    Superior|Guiana Francesa|         2007|\n",
      "|     Albert Leef|    Superior|        Bolívia|         2000|\n",
      "|      Daryl Page| Fundamental|           Peru|         2008|\n",
      "|      Anita Ross| Fundamental|        Bolívia|         2007|\n",
      "|     Sandra Todd| Fundamental|        Bolívia|         2005|\n",
      "|    Juliet Liles|       Médio|        Equador|         2000|\n",
      "|Cristina Sheston| Fundamental|        Uruguai|         2002|\n",
      "+----------------+------------+---------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_select = df_nomes.select(\"*\").where(col(\"AnoNascimento\") >= 2000)\n",
    "df_select.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Trazer linhas da tabela com pessoas do século 2000 utilizando spark sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+---------------+-------------+\n",
      "|           Nomes|Escolaridade|           Pais|AnoNascimento|\n",
      "+----------------+------------+---------------+-------------+\n",
      "|     Joy Bennett| Fundamental|         Brasil|         2007|\n",
      "| Howard Lazarine|    Superior|         Brasil|         2003|\n",
      "|    Leroy Strahl|    Superior|Guiana Francesa|         2008|\n",
      "|    David Medina|    Superior|       Paraguai|         2007|\n",
      "|     Albert Leef|       Médio|         Guiana|         2000|\n",
      "|      Daryl Page|       Médio|       Paraguai|         2008|\n",
      "|      Anita Ross|       Médio|Guiana Francesa|         2007|\n",
      "|     Sandra Todd|       Médio|        Equador|         2005|\n",
      "|    Juliet Liles|       Médio|       Paraguai|         2000|\n",
      "|Cristina Sheston| Fundamental|           Peru|         2002|\n",
      "+----------------+------------+---------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_nomes.createOrReplaceTempView(\"pessoas\")\n",
    "spark.sql(\"select * from pessoas where AnoNascimento >= 2000\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Contar a quantidade de pessoas da geração Millennials existente utilizando as funções do pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de pessoas da geração Millennials: 2308171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "geracao_millennials = df_nomes.select(col(\"*\")).where((col(\"AnoNascimento\") >= 1980) & (col(\"AnoNascimento\") <= 1994)).count()\n",
    "\n",
    "print(f\"Quantidade de pessoas da geração Millennials: {geracao_millennials}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Contar a quantidade de pessoas da geração Millennials existente utilizando as funções do spark sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 79:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|geracao_millennials|\n",
      "+-------------------+\n",
      "|            2308171|\n",
      "+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) as geracao_millennials from pessoas where AnoNascimento >= 1980 and AnoNascimento <= 1994\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Apresentar quantidade de pessoas de cada geração agrupadas por país, em seguida ordernar a saia por país, geração e quantidade, todos em orderm crescente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 73:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+----------+\n",
      "|pais     |geracao     |quantidade|\n",
      "+---------+------------+----------+\n",
      "|Argentina|Baby Boomers|236904    |\n",
      "|Argentina|Geração X   |177924    |\n",
      "|Argentina|Geração Z   |177962    |\n",
      "|Argentina|Millennials |177522    |\n",
      "|Bolívia  |Baby Boomers|236039    |\n",
      "|Bolívia  |Geração X   |177689    |\n",
      "|Bolívia  |Geração Z   |177197    |\n",
      "|Bolívia  |Millennials |178152    |\n",
      "|Brasil   |Baby Boomers|236937    |\n",
      "|Brasil   |Geração X   |177663    |\n",
      "|Brasil   |Geração Z   |177798    |\n",
      "|Brasil   |Millennials |177674    |\n",
      "|Chile    |Baby Boomers|237163    |\n",
      "|Chile    |Geração X   |178325    |\n",
      "|Chile    |Geração Z   |176954    |\n",
      "|Chile    |Millennials |177512    |\n",
      "|Colômbia |Baby Boomers|236928    |\n",
      "|Colômbia |Geração X   |177964    |\n",
      "|Colômbia |Geração Z   |177552    |\n",
      "|Colômbia |Millennials |176903    |\n",
      "+---------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_geracoes = spark.sql(\"SELECT pais, CASE WHEN AnoNascimento >= 1944 AND AnoNascimento <= 1964 THEN 'Baby Boomers'WHEN AnoNascimento >= 1965 AND AnoNascimento <= 1979 THEN 'Geração X' WHEN AnoNascimento >= 1980 AND AnoNascimento <= 1994 THEN 'Millennials' WHEN AnoNascimento >= 1995 AND AnoNascimento <= 2015 THEN 'Geração Z' END AS geracao, count(*) as quantidade FROM  Pessoas group by pais, geracao\")\n",
    "df_geracoes.orderBy(col(\"pais\"), col(\"geracao\"), col(\"quantidade\")).show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
